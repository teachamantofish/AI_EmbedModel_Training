#!/usr/bin/env python3
"""
MEMORY-OPTIMIZED TRAINING TEST
==============================

Test aggressive memory management for Qwen3-4B on RX 580.
Focus on preventing 100% memory usage.
"""

import torch
import torch_directml
import logging
import gc
import os
from sentence_transformers import SentenceTransformer, InputExample
import torch.nn.functional as F
from torch.optim import AdamW

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def aggressive_memory_cleanup():
    """Ultra-aggressive memory cleanup."""
    gc.collect()
    try:
        torch_directml.empty_cache()
    except:
        pass
    try:
        torch.cuda.empty_cache()
    except:
        pass

def memory_optimized_test():
    """Test memory-optimized training approach."""
    logger.info("üßπ MEMORY-OPTIMIZED TRAINING TEST")
    logger.info("üéØ Goal: Prevent 100% memory usage with Qwen3-4B")
    
    try:
        # 1. GPU Setup with minimal memory footprint
        gpu_device = torch_directml.device()
        logger.info(f"‚úÖ DirectML device: {gpu_device}")
        
        # 2. Load model with minimal memory impact
        logger.info("üì¶ Loading Qwen3-4B model (memory optimized)...")
        model = SentenceTransformer('Qwen/Qwen3-Embedding-4B')
        logger.info("‚úÖ Model loaded on CPU")
        
        # 3. Create tiny test dataset (minimal memory)
        logger.info("üìä Creating minimal test dataset...")
        train_examples = [
            InputExample(texts=["anchor text 1", "positive text 1", "negative text 1"]),
            InputExample(texts=["anchor text 2", "positive text 2", "negative text 2"])
        ]
        logger.info(f"‚úÖ Test dataset: {len(train_examples)} examples")
        
        # 4. Setup optimizer (only parameters, not model)
        optimizer = AdamW(model.parameters(), lr=5e-6, weight_decay=0.01)
        logger.info("‚úÖ Optimizer created")
        
        # 5. Test one training step with extreme memory management
        logger.info("üöÄ Testing ONE training step with memory cleanup...")
        
        model.train()
        
        # Process just ONE example with maximum cleanup
        example = train_examples[0]
        
        # Extract texts
        anchor_text = [example.texts[0]]
        positive_text = [example.texts[1]]
        negative_text = [example.texts[2]]
        
        logger.info("   üìù Tokenizing (CPU)...")
        anchor_inputs = model.tokenize(anchor_text)
        positive_inputs = model.tokenize(positive_text)
        negative_inputs = model.tokenize(negative_text)
        
        logger.info("   üß† Forward pass (CPU)...")
        # Forward pass on CPU
        anchor_emb = model(anchor_inputs)['sentence_embedding']
        positive_emb = model(positive_inputs)['sentence_embedding']
        negative_emb = model(negative_inputs)['sentence_embedding']
        
        logger.info("   üöÄ Moving to GPU for loss...")
        # Move to GPU for loss computation only
        anchor_emb_gpu = anchor_emb.to(gpu_device)
        positive_emb_gpu = positive_emb.to(gpu_device)
        negative_emb_gpu = negative_emb.to(gpu_device)
        
        # Clean up CPU tensors immediately
        del anchor_emb, positive_emb, negative_emb
        del anchor_inputs, positive_inputs, negative_inputs
        aggressive_memory_cleanup()
        
        logger.info("   üìä Computing loss on GPU...")
        loss = F.triplet_margin_loss(anchor_emb_gpu, positive_emb_gpu, negative_emb_gpu, margin=1.0)
        
        logger.info("   ‚¨ÖÔ∏è Backward pass...")
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        logger.info(f"   ‚úÖ Loss: {loss.item():.4f}")
        
        # Ultra cleanup after step
        del anchor_emb_gpu, positive_emb_gpu, negative_emb_gpu, loss
        aggressive_memory_cleanup()
        
        logger.info("üéâ MEMORY TEST PASSED!")
        logger.info("üí° One training step completed with memory management")
        logger.info("üåô Ready for overnight training with this approach")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Memory test failed: {e}")
        return False
    
    finally:
        # Final cleanup
        try:
            del model, optimizer
        except:
            pass
        aggressive_memory_cleanup()

if __name__ == "__main__":
    success = memory_optimized_test()
    if success:
        print("\nüåô READY FOR OVERNIGHT TRAINING!")
        print("   Memory management approach validated")
        print("   Run full training with confidence")
    else:
        print("\n‚ùå MEMORY ISSUES DETECTED")
        print("   Need to adjust approach before overnight run")